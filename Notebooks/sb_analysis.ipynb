{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e07a9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE ---------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from urllib import request\n",
    "import re\n",
    "import time\n",
    "\n",
    "# VISUALIZATIONS -----------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# MODELS -------------------------------\n",
    "\n",
    "\n",
    "# STYLES & CONFIGURATIONS ---------------\n",
    "sns.set_theme(style='darkgrid')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "0a3d766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_strings_after_specific_element(input_list, specific_string):\n",
    "    \"\"\"\n",
    "    Function to end the scraping of URLs after the last URL of the regular seasons\n",
    "    \"\"\"\n",
    "    if specific_string in input_list:\n",
    "        index = input_list.index(specific_string)\n",
    "        del input_list[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ae36139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(url=\"https://spongebob.fandom.com/wiki/List_of_transcripts\"):\n",
    "    \"\"\"\n",
    "    Function to get all URLs from List of Transcripts page to scrape each transcript\n",
    "    \"\"\"\n",
    "\n",
    "    page = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"\\/transcript\")}):\n",
    "        links.append(link.get('href'))\n",
    "    specific_string = '/wiki/Sandy,_Help_Us!/transcript'\n",
    "    remove_strings_after_specific_element(links, specific_string)\n",
    "    \n",
    "    link_list = [\"https://spongebob.fandom.com/\" + x for x in links]\n",
    "    \n",
    "    for link in link_list:\n",
    "    ep_names = [x[34:] for x in link_list]\n",
    "    \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f7bdb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(row):\n",
    "    '''\n",
    "    Function to get only the transcript text from the given URL and return the final output as a string\n",
    "    '''\n",
    "    url = row\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    soup_sp = \" \".join([a.get_text().strip() for a in soup])\n",
    "    soup_sn = soup_sp.replace('\\n', ' ')\n",
    "    soup_st = soup_sn.replace('\\t', '')\n",
    "    \n",
    "    begin_str='which aired on '\n",
    "    ending_str= 'Categories'\n",
    "    res = soup_st.split(begin_str, 1)\n",
    "    res2 = ''.join(res[-1])\n",
    "    end = res2.split(ending_str)[0]\n",
    "    time.sleep(1)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "5d73f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of the URLs\n",
    "links_df = pd.DataFrame(link_list, columns=['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "ac6c5887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get name of episode + '/transcript' and add it to dataframe\n",
    "links_df['ep_name'] = [link[34:] for link in link_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "650b1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column for transcript and apply get_transcript\n",
    "links_df['transcript'] = links_df['URL'].apply(get_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "c82ac440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to CSV file\n",
    "# links_df.to_csv('transcript.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6f58b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Notes\n",
    "* How should we clean the transcripts and preprocess them for analysis?\n",
    "* What analysis techniques could we apply?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
